[
    {
        "question": "The Mapper implementation processes one line at a time via _________ method.",
        "options": [
            "a) map",
            "b) reduce",
            "c) mapper",
            "d) reducer"
        ],
        "answer": "a",
        "explanation": "The Mapper outputs are sorted and then partitioned per Reducer."
    },
    {
        "question": "Point out the correct statement.",
        "options": [
            "a) Mapper maps input key/value pairs to a set of intermediate key/value pairs",
            "b) Applications typically implement the Mapper and Reducer interfaces to provide the map and reduce methods",
            "c) Mapper and Reducer interfaces form the core of the job",
            "d) None of the mentioned"
        ],
        "answer": "d",
        "explanation": "The transformed intermediate records do not need to be of the same type as the input records."
    },
    {
        "question": "The Hadoop MapReduce framework spawns one map task for each __________ generated by the InputFormat for the job.",
        "options": [
            "a) OutputSplit",
            "b) InputSplit",
            "c) InputSplitStream",
            "d) All of the mentioned"
        ],
        "answer": "b",
        "explanation": "Mapper implementations are passed the JobConf for the job via the JobConfigurable.configure(JobConf) method and override it to initialize themselves."
    },
    {
        "question": "Users can control which keys (and hence records) go to which Reducer by implementing a custom?",
        "options": [
            "a) Partitioner",
            "b) OutputSplit",
            "c) Reporter",
            "d) All of the mentioned"
        ],
        "answer": "a",
        "explanation": "Users can control the grouping by specifying a Comparator via JobConf.setOutputKeyComparatorClass(Class)."
    },
    {
        "question": "Point out the wrong statement.",
        "options": [
            "a) The Mapper outputs are sorted and then partitioned per Reducer",
            "b) The total number of partitions is the same as the number of reduce tasks for the job",
            "c) The intermediate, sorted outputs are always stored in a simple (key-len, key, value-len, value) format",
            "d) None of the mentioned"
        ],
        "answer": "d",
        "explanation": "All intermediate values associated with a given output key are subsequently grouped by the framework, and passed to the Reducer(s) to determine the final output."
    },
    {
        "question": "Applications can use the ____________ to report progress and set application-level status messages.",
        "options": [
            "a) Partitioner",
            "b) OutputSplit",
            "c) Reporter",
            "d) All of the mentioned"
        ],
        "answer": "c",
        "explanation": "Reporter is also used to update Counters, or just indicate that they are alive."
    },
    {
        "question": "The right level of parallelism for maps seems to be around _________ maps per-node.",
        "options": [
            "a) 1-10",
            "b) 10-100",
            "c) 100-150",
            "d) 150-200"
        ],
        "answer": "b",
        "explanation": "Task setup takes a while, so it is best if the maps take at least a minute to execute."
    },
    {
        "question": "The number of reduces for the job is set by the user via _________",
        "options": [
            "a) JobConf.setNumTasks(int)",
            "b) JobConf.setNumReduceTasks(int)",
            "c) JobConf.setNumMapTasks(int)",
            "d) All of the mentioned"
        ],
        "answer": "b",
        "explanation": "Reducer has 3 primary phases: shuffle, sort and reduce."
    },
    {
        "question": "The framework groups Reducer inputs by key in _________ stage.",
        "options": [
            "a) sort",
            "b) shuffle",
            "c) reduce",
            "d) none of the mentioned"
        ],
        "answer": "a",
        "explanation": "The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged."
    },
    {
        "question": "The output of the reduce task is typically written to the FileSystem via _____________",
        "options": [
            "a) OutputCollector.collect",
            "b) OutputCollector.get",
            "c) OutputCollector.receive",
            "d) OutputCollector.put"
        ],
        "answer": "a",
        "explanation": "The output of the Reducer is not sorted."
    }
]